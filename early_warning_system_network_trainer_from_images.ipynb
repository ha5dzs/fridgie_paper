{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1gzTeFXi-fL0"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT:\n",
        "# First of all, copy the 'use_this_in_the_tensorflow_code.zip' file into this environment.\n",
        "# You have to do this every time you are running this in Colab, because it discards everything after you close it.\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "\n",
        "try:\n",
        "  # Open the file\n",
        "  the_zip_file = zipfile.ZipFile('./use_this_in_the_tensorflow_code.zip', 'r')\n",
        "  # Extract the contents to the same directory as this code is running in.\n",
        "  the_zip_file.extractall('./')\n",
        "  # Close the file\n",
        "  the_zip_file.close()\n",
        "except:\n",
        "  print(\"The zip file could not be opened. Make sure you upload it to the same directory where this code is.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we define our directories. All manual, for clarity.\n",
        "\n",
        "# Training data set\n",
        "\n",
        "training_source_path = './fridgie_data_training'\n",
        "\n",
        "# Check a condition\n",
        "train_dir_check_airflow = os.path.join(training_source_path + '/check_airflow')\n",
        "\n",
        "try:\n",
        "  print(f\"There are {len(os.listdir(train_dir_check_airflow))} images in the training data check_airflow.\")\n",
        "except:\n",
        "  print(\"Looks like the training data wasn't extracted properly.\")\n",
        "\n",
        "# Testing data set\n",
        "\n",
        "testing_source_path = './fridgie_data_testing'\n",
        "\n",
        "test_dir_high_heatload = os.path.join(testing_source_path + '/high_heatload')\n",
        "\n",
        "try:\n",
        "  print(f\"There are {len(os.listdir(test_dir_high_heatload))} images in the testing data high_heatload.\")\n",
        "except:\n",
        "  print(\"Looks like the testing data wasn't extracted properly.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG_0xUOZCs8e",
        "outputId": "c82fe847-e5f0-4653-eb66-68ce6cca025c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 10000 images in the training data check_airflow.\n",
            "There are 2500 images in the testing data high_heatload.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actual machine learing thing: the neural network we have\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# This is the model definition\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Since our images are 5-by-1 pixels, this can be fairly simple.\n",
        "    tf.keras.layers.Flatten(), # Input layer. This is not really a layer, but it strips extra dimensions from the data\n",
        "    tf.keras.layers.Dense(94, activation = tf.nn.relu), # Hidden layer\n",
        "    tf.keras.layers.Dense(6, activation = tf.nn.softmax) # Output layer, must match the number of labels\n",
        "])\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'rmsprop',\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "-L9MsWCcGZS-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we are reducing the temperature and duty cycle measurements as image data, we will use the image data generator.\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "no_of_batches = 32\n",
        "\n",
        "train_gen = ImageDataGenerator( rescale = 1.0/255. ) # Image data is encoded as 8-bit values.\n",
        "\n",
        "train_generator = train_gen.flow_from_directory(\n",
        "    training_source_path,\n",
        "    batch_size = no_of_batches\n",
        ")\n",
        "\n",
        "test_gen = ImageDataGenerator( rescale = 1.0/255. ) # Same as above\n",
        "\n",
        "test_generator = test_gen.flow_from_directory(\n",
        "    testing_source_path,\n",
        "    batch_size = no_of_batches\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKp70a1cLRSw",
        "outputId": "f4644ac6-d5ca-441b-dc59-1007e4e19422"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 60000 images belonging to 6 classes.\n",
            "Found 15000 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since training can be a long thing, it is worthwhile to do a callback function\n",
        "# This stops training when we reached the required accuracy when training and validating the data\n",
        "# Also, training for too long may have a negative effect on performance\n",
        "\n",
        "class are_we_there_yet(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    # We can separate the training and validation accuracy.\n",
        "    required_training_accuracy = 0.99\n",
        "    required_validation_accuracy = 0.99\n",
        "    # This runs at every epoch's end, and checks whether we can stop.\n",
        "    if(logs.get('accuracy') >= required_training_accuracy and logs.get('val_accuracy') >= required_validation_accuracy):\n",
        "      print(\"\\n\\nWe got it, stopping training.\\n\\n\")\n",
        "      self.model.stop_training = True"
      ],
      "metadata": {
        "id": "RXFG_BKjShqj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we do the training\n",
        "\n",
        "no_of_epochs = 50 # This is way too many, the callback function will stop training when we are done.\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs = no_of_epochs,\n",
        "    verbose = 1,\n",
        "    validation_data = test_generator,\n",
        "    callbacks = [are_we_there_yet()]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egyVwRpGNxK0",
        "outputId": "6be631ee-cefa-4c08-e068-cd78e88d9644"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1875/1875 [==============================] - 112s 59ms/step - loss: 6.7484 - accuracy: 0.8037 - val_loss: 0.0399 - val_accuracy: 0.9929\n",
            "Epoch 2/50\n",
            "1875/1875 [==============================] - 103s 55ms/step - loss: 0.7509 - accuracy: 0.9529 - val_loss: 2.3738 - val_accuracy: 0.8136\n",
            "Epoch 3/50\n",
            "1875/1875 [==============================] - 108s 58ms/step - loss: 0.3903 - accuracy: 0.9743 - val_loss: 0.2614 - val_accuracy: 0.9698\n",
            "Epoch 4/50\n",
            "1875/1875 [==============================] - 104s 55ms/step - loss: 0.2579 - accuracy: 0.9805 - val_loss: 0.0293 - val_accuracy: 0.9953\n",
            "Epoch 5/50\n",
            "1875/1875 [==============================] - 108s 58ms/step - loss: 0.2099 - accuracy: 0.9840 - val_loss: 0.0529 - val_accuracy: 0.9873\n",
            "Epoch 6/50\n",
            "1875/1875 [==============================] - 101s 54ms/step - loss: 0.1475 - accuracy: 0.9828 - val_loss: 0.0118 - val_accuracy: 0.9976\n",
            "Epoch 7/50\n",
            "1875/1875 [==============================] - 109s 58ms/step - loss: 0.1283 - accuracy: 0.9865 - val_loss: 0.0078 - val_accuracy: 0.9989\n",
            "Epoch 8/50\n",
            "1875/1875 [==============================] - 109s 58ms/step - loss: 0.1182 - accuracy: 0.9893 - val_loss: 0.0104 - val_accuracy: 0.9980\n",
            "Epoch 9/50\n",
            "1875/1875 [==============================] - 112s 60ms/step - loss: 0.1174 - accuracy: 0.9891 - val_loss: 0.0097 - val_accuracy: 0.9986\n",
            "Epoch 10/50\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0880 - accuracy: 0.9907\n",
            "\n",
            "We got it, stopping training.\n",
            "\n",
            "\n",
            "1875/1875 [==============================] - 105s 56ms/step - loss: 0.0880 - accuracy: 0.9907 - val_loss: 0.0187 - val_accuracy: 0.9975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "\n",
        "model.save_weights('./saved_model') # Save the file in the same working directory.\n",
        "\n",
        "# ...and at this point, you can process the model further, as you wish.\n",
        "\n"
      ],
      "metadata": {
        "id": "jv47KXl4KsQH"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}